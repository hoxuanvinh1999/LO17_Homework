{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f29cb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f550042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /home/tranquochung/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /home/tranquochung/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     /home/tranquochung/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /home/tranquochung/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/tranquochung/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('genesis')\n",
    "nltk.download('inaugural')\n",
    "nltk.download('nps_chat')\n",
    "nltk.download('webtext')\n",
    "nltk.download('treebank')\n",
    "\n",
    "from nltk.corpus import udhr\n",
    "from nltk.book import *\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d712bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('tbbt/s3/txt/tbbts03e01.txt','r')\n",
    "dataTxt = f.read()\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d74260f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  nltk \n",
    "import codecs \n",
    "import pickle\n",
    "\n",
    "def getWords(filemname):\n",
    "    f = codecs.open(filename, 'r', 'utf-8')\n",
    "    text = f.read().lower()\n",
    "#     l = nltk.regexp_tokenize(text, \"\\w+\")\n",
    "    f.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8f522017",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tbbt/s3/txt/tbbts03e01.txt'\n",
    "\n",
    "dataTxt = getWords(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe62e4",
   "metadata": {},
   "source": [
    "## 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f24ac8",
   "metadata": {},
   "source": [
    "### Découpage en phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98056c59",
   "metadata": {},
   "source": [
    "Afficher le nombre de phases ainsi que les phases correspontdants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0c41bc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences in text: 306\n",
      "\n",
      "\n",
      "[\"i just want you both tknow, when i publish my findings, i won't forget your contributions.\", '- great.', '- thanks.', \"of course, ian't mention you in my nobel acceptance speech, but when i get around to writing my memoirs, you can expect a very effusive footnote and perhaps a signed copy.\", '- we have to tell him.', '- tell me what?', 'damn his vulcan hearing.', \"you fellows are planning a party for me, aren't you?\", 'okay, sheldon, sit down.', 'if there\\'s going to be a them i should let you know that i don\\'t care for luau, toga or \" under the sea. \"']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "print(\"number of sentences in text:\",len(sent_tokenize(dataTxt)))\n",
    "print(\"\\n\")\n",
    "sentences=sent_tokenize(dataTxt)[:10]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0775d1b",
   "metadata": {},
   "source": [
    "### Découpage en mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54a45460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 3058\n",
      "\n",
      "\n",
      "['i', 'just', 'want', 'you', 'both', 'tknow', ',', 'when', 'i', 'publish', 'my', 'findings', ',', 'i', 'wo', \"n't\", 'forget', 'your', 'contributions', '.', '-', 'great', '.', '-', 'thanks', '.', 'of', 'course', ',', 'ia', \"n't\", 'mention', 'you', 'in', 'my', 'nobel', 'acceptance', 'speech', ',', 'but', 'when', 'i']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens:\",len(word_tokenize(dataTxt)))\n",
    "print(\"\\n\")\n",
    "words = word_tokenize(dataTxt)[:42]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6ff1d",
   "metadata": {},
   "source": [
    "### Découpage en phrases ensuite en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28363c52",
   "metadata": {},
   "source": [
    "c'est-à-dire en redécoupant chaque phrase en mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09bc344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "345dc456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'just', 'want', 'you', 'both', 'tknow', ',', 'when', 'i', 'publish', 'my', 'findings', ',', 'i', 'wo', \"n't\", 'forget', 'your', 'contributions', '.'], ['-', 'great', '.'], ['-', 'thanks', '.'], ['of', 'course', ',', 'ia', \"n't\", 'mention', 'you', 'in', 'my', 'nobel', 'acceptance', 'speech', ',', 'but', 'when', 'i', 'get', 'around', 'to', 'writing', 'my', 'memoirs', ',', 'you', 'can', 'expect', 'a', 'very', 'effusive', 'footnote', 'and', 'perhaps', 'a', 'signed', 'copy', '.'], ['-', 'we', 'have', 'to', 'tell', 'him', '.'], ['-', 'tell', 'me', 'what', '?']]\n"
     ]
    }
   ],
   "source": [
    "word_sent = [word_tokenize(s) for s in sent_tokenize(dataTxt)]\n",
    "print(word_sent[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8942d0",
   "metadata": {},
   "source": [
    "## Part-Of-Speech Tagging (POS Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7976f8",
   "metadata": {},
   "source": [
    "Pos tagging pour les phrases en utilisant la fonction nltk.pos_tag_sents sur le object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4d8476e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tranquochung/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/tranquochung/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5136040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nltk.pos_tag_sents(word_sent, tagset=\"universal\")[:2])\n",
    "\n",
    "pos_tag_sents = nltk.pos_tag_sents(word_sent, tagset=\"universal\")\n",
    "verb_words = []\n",
    "noun_words = []\n",
    "adj_words  = []\n",
    "\n",
    "for pos_tag_sent in  pos_tag_sents:\n",
    "     for id in range(0,len(pos_tag_sent)):\n",
    "        if (pos_tag_sent[id][1] == 'VERB'):\n",
    "#             print(pos_tag_sent[id][0])\n",
    "            verb_words.append(pos_tag_sent[id][0])\n",
    "        if (pos_tag_sent[id][1] == 'NOUN'):\n",
    "#             print(pos_tag_sent[id][0])\n",
    "            noun_words.append(pos_tag_sent[id][0])\n",
    "        if (pos_tag_sent[id][1] == 'ADJ'):\n",
    "#             print(pos_tag_sent[id][0])\n",
    "            adj_words.append(pos_tag_sent[id][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ef728",
   "metadata": {},
   "source": [
    "## Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333079aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "28c1e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/tranquochung/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/tranquochung/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "55affbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lem_noun = []\n",
    "lem_verb = []\n",
    "lem_adj  = []\n",
    "\n",
    "\n",
    "for n in noun_words : \n",
    "    lem_noun.append(lemmatizer.lemmatize(n,'n'))\n",
    "\n",
    "for v in verb_words :\n",
    "    lem_verb.append(lemmatizer.lemmatize(v,'v'))\n",
    "\n",
    "for adj in adj_words :\n",
    "    lem_adj.append(lemmatizer.lemmatize(adj,'a'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "714d0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmtisation_words(type, nb_words):\n",
    "    \n",
    "    print(\"Afficher la lemmatisation de quelques nouns :\")\n",
    "    for i in range(nb_words):\n",
    "        if type == 'NOUN':\n",
    "            print(noun_words[i], \" -----> \",lem_noun[i])\n",
    "        elif type == 'VERB':\n",
    "            print(verb_words[i], \" -----> \",lem_verb[i])\n",
    "        elif type == 'ADJ':\n",
    "            print(adj_words[i], \" -----> \",lem_adj[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1b611e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afficher la lemmatisation de quelques nouns :\n",
      "i  ----->  i\n",
      "i  ----->  i\n",
      "findings  ----->  finding\n",
      "i  ----->  i\n",
      "contributions  ----->  contribution\n",
      "thanks  ----->  thanks\n",
      "course  ----->  course\n",
      "mention  ----->  mention\n",
      "acceptance  ----->  acceptance\n",
      "speech  ----->  speech\n",
      "memoirs  ----->  memoir\n",
      "footnote  ----->  footnote\n",
      "copy  ----->  copy\n",
      "tell  ----->  tell\n",
      "hearing  ----->  hearing\n",
      "fellows  ----->  fellow\n",
      "party  ----->  party\n",
      "okay  ----->  okay\n",
      "sheldon  ----->  sheldon\n",
      "i  ----->  i\n"
     ]
    }
   ],
   "source": [
    "get_lemmtisation_words('NOUN', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8f8527d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afficher la lemmatisation de quelques nouns :\n",
      "want  ----->  want\n",
      "tknow  ----->  tknow\n",
      "publish  ----->  publish\n",
      "wo  ----->  wo\n",
      "forget  ----->  forget\n",
      "ia  ----->  ia\n",
      "get  ----->  get\n",
      "writing  ----->  write\n",
      "can  ----->  can\n",
      "expect  ----->  expect\n",
      "signed  ----->  sign\n",
      "have  ----->  have\n",
      "tell  ----->  tell\n",
      "damn  ----->  damn\n",
      "are  ----->  be\n",
      "planning  ----->  plan\n",
      "are  ----->  be\n",
      "sit  ----->  sit\n",
      "'s  ----->  's\n",
      "going  ----->  go\n"
     ]
    }
   ],
   "source": [
    "get_lemmtisation_words('VERB', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4e02ac61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afficher la lemmatisation de quelques nouns :\n",
      "great  ----->  great\n",
      "nobel  ----->  nobel\n",
      "i  ----->  i\n",
      "effusive  ----->  effusive\n",
      "vulcan  ----->  vulcan\n",
      "north  ----->  north\n",
      "first  ----->  first\n",
      "few  ----->  few\n",
      "magnetic  ----->  magnetic\n",
      "obnoxious  ----->  obnoxious\n",
      "giant  ----->  giant\n",
      "gentle  ----->  gentle\n",
      "first  ----->  first\n",
      "positive  ----->  positive\n",
      "happy  ----->  happy\n",
      "much  ----->  much\n",
      "paradigm-shifting  ----->  paradigm-shifting\n",
      "static  ----->  static\n",
      "electric  ----->  electric\n",
      "only  ----->  only\n"
     ]
    }
   ],
   "source": [
    "get_lemmtisation_words('ADJ', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e19fa1b",
   "metadata": {},
   "source": [
    "### Reconnaissance d’entités nommées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c03952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2613913e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_web_sm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [175]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m displacy\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01men_core_web_sm\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'en_core_web_sm'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
